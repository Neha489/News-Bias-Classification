{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f394df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    " \n",
    "# ==== SETUP LOGGING ====\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    " \n",
    "# ==== CUDA/GPU CONFIGURATION ====\n",
    "def configure_cuda():\n",
    "    if not torch.cuda.is_available():\n",
    "        logger.error(\"CUDA not available! Exiting.\")\n",
    "        return False\n",
    "    logger.info(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    logger.info(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    logger.info(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    logger.info(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    return True\n",
    " \n",
    "# ==== DATA PROCESSING ====\n",
    "def process_biased_words(x):\n",
    "    if not isinstance(x, str):\n",
    "        return ''\n",
    "    if x.startswith('['):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                return ' '.join(str(item) for item in parsed)\n",
    "            return str(parsed)\n",
    "        except:\n",
    "            return ''\n",
    "    return x\n",
    " \n",
    "# ==== CUSTOM DATASET ====\n",
    "class BiasDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    " \n",
    "# ==== PYTORCH BERT PIPELINE ====\n",
    "def run_pytorch_bert_pipeline(\n",
    "    csv_path,\n",
    "    output_path=\"bias_classifier_model\",\n",
    "    sample_size=None,\n",
    "    max_len=96,\n",
    "    batch_size=16,\n",
    "    epochs=3,\n",
    "    validation_split=0.1\n",
    "):\n",
    "    start_time = time.time()\n",
    "    logger.info(f\"Starting PyTorch BERT pipeline at {time.strftime('%H:%M:%S')}\")\n",
    " \n",
    "    # Configure CUDA\n",
    "    if not configure_cuda():\n",
    "        logger.error(\"CUDA setup failed. Exiting.\")\n",
    "        return None\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    " \n",
    "    # Load data\n",
    "    logger.info(f\"Loading data from {csv_path}\")\n",
    "    df = pd.read_csv(csv_path, nrows=sample_size) if sample_size else pd.read_csv(csv_path)\n",
    "    logger.info(f\"Loaded {len(df)} rows\")\n",
    " \n",
    "    # Process data\n",
    "    logger.info(\"Processing data\")\n",
    "    if 'biased_words' in df.columns:\n",
    "        df['biased_words'] = df['biased_words'].fillna('').apply(process_biased_words)\n",
    "    else:\n",
    "        df['biased_words'] = ''\n",
    "    for col in ['text', 'topic', 'article']:\n",
    "        df[col] = df.get(col, '').fillna('').astype(str)\n",
    "    df['full_input'] = df['text'] + \" \" + df['topic'] + \" \" + df['article'] + \" \" + df['biased_words']\n",
    " \n",
    "    # Process labels\n",
    "    if 'type' not in df.columns:\n",
    "        logger.error(\"Column 'type' not found\")\n",
    "        return None\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df['type'])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    logger.info(f\"Classes: {label_encoder.classes_}\")\n",
    " \n",
    "    # Split data\n",
    "    logger.info(\"Splitting data\")\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "        df['full_input'].tolist(), y,\n",
    "        test_size=validation_split, stratify=y, random_state=42\n",
    "    )\n",
    " \n",
    "    # Initialize tokenizer\n",
    "    logger.info(\"Loading BERT tokenizer\")\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    " \n",
    "    # Create datasets\n",
    "    logger.info(\"Creating datasets\")\n",
    "    train_dataset = BiasDataset(train_texts, train_labels, tokenizer, max_len)\n",
    "    val_dataset = BiasDataset(val_texts, val_labels, tokenizer, max_len)\n",
    " \n",
    "    # Initialize model\n",
    "    logger.info(\"Loading BERT model\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=num_classes\n",
    "    ).to(device)\n",
    " \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./checkpoints\",\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=len(train_labels) // batch_size // 2,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        fp16=True,  # Enable mixed precision for GPU\n",
    "        report_to=\"none\"\n",
    "    )\n",
    " \n",
    "    # Define compute metrics\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        accuracy = (predictions == labels).mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    " \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    " \n",
    "    # Train\n",
    "    logger.info(\"Starting training\")\n",
    "    trainer.train()\n",
    " \n",
    "    # Evaluate\n",
    "    logger.info(\"Evaluating model\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    logger.info(f\"Validation Accuracy: {eval_results['eval_accuracy']}\")\n",
    " \n",
    "    # Classification report\n",
    "    logger.info(\"Generating classification report\")\n",
    "    predictions = trainer.predict(val_dataset)\n",
    "    pred_classes = np.argmax(predictions.predictions, axis=-1)\n",
    "    logger.info(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, pred_classes, target_names=label_encoder.classes_))\n",
    " \n",
    "    # Save model\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    logger.info(f\"Saving model to {output_path}\")\n",
    "    model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    import pickle\n",
    "    with open(f\"{output_path}/label_encoder.pkl\", \"wb\") as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    " \n",
    "    training_time = time.time() - start_time\n",
    "    logger.info(f\"Completed in {training_time // 3600}h {(training_time % 3600) // 60}m\")\n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'label_encoder': label_encoder,\n",
    "        'accuracy': eval_results['eval_accuracy']\n",
    "    }\n",
    " \n",
    "# ==== ENTRY POINT ====\n",
    "if __name__ == \"__main__\":\n",
    "    # Verify CUDA\n",
    "    if not configure_cuda():\n",
    "        logger.error(\"CUDA not available. Exiting.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Run pipeline\n",
    "    result = run_pytorch_bert_pipeline(csv_path=\"balanced_data.csv\")\n",
    "    logger.info(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c8ba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    " \n",
    " \n",
    "model_path = \"bias_classifier_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path,local_files_only=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "test_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    " \n",
    "samples = [\n",
    "[\"\"\"As wildfires rage across California, floods displace thousands in the Midwest, and heatwaves scorch cities from Texas to New York, the evidence is undeniable: the climate crisis is no longer a distant threat—it’s here. And yet, as communities suffer and ecosystems collapse, fossil fuel corporations continue to post record-breaking profits, protected by conservative politicians and a global system rigged in their favor.\n",
    "In 2024 alone, the five largest oil companies reported over $200 billion in profits. Instead of investing in renewable energy or helping vulnerable communities transition to a green economy, these corporations funneled billions into stock buybacks and executive bonuses. Their message is clear: profits come before people, and the planet can burn so long as the shareholders stay rich.\n",
    "Even more alarming is the political shielding they receive from right-wing lawmakers, many of whom deny climate science altogether. Republican leaders in Congress have repeatedly blocked climate legislation, gutted the Environmental Protection Agency’s regulatory powers, and prioritized drilling permits over clean air and water.\n",
    "Meanwhile, climate activists—many of them youth, Indigenous leaders, and marginalized communities—continue to face police repression, surveillance, and criminalization. Peaceful protesters at pipeline sites are arrested, while oil spills and environmental destruction go unpunished.\n",
    "We need a Green New Deal-level transformation: bold investments in wind, solar, and green infrastructure; the creation of millions of unionized green jobs; and climate reparations for communities hit hardest by pollution and environmental racism.\n",
    "The time for delay is over. The time to act is now.\"\"\"\n",
    "],\n",
    "[\"\"\"The United States thrives when government steps back and lets free enterprise lead. In recent years, however, progressive lawmakers have increasingly pushed for regulation, redistribution, and intervention that stifles innovation and discourages hard work.\n",
    "From overreaching environmental mandates to government-controlled healthcare proposals, the left continues to champion policies that prioritize bureaucracy over results. These moves are not only anti-business—they’re anti-American.\n",
    "America's economic engine runs best when the private sector is free to create, compete, and grow. Small business owners across the country are already struggling with inflation and labor shortages—problems worsened by excessive government interference and rising taxes.\n",
    "We must return to policies that reward productivity, protect property rights, and uphold free-market values. Deregulation, tax reform, and energy independence will not only restore our economy—they’ll renew our national spirit.\n",
    "\"\"\"],\n",
    "[\"\"\"As artificial intelligence tools become increasingly integrated into everyday life—from health diagnostics to criminal justice systems—Democratic and Republican lawmakers alike are recognizing the need for clear regulatory frameworks.\n",
    "A bipartisan group in Congress recently introduced the American AI Responsibility Act, aiming to address transparency, data privacy, and algorithmic bias. While the bill doesn’t go as far as some activists demand, it marks an important step toward balancing innovation with accountability.\n",
    "Tech CEOs have expressed cautious support, stating that some regulation is needed to maintain public trust, but they warn against overregulation that could drive development offshore.\n",
    "Experts agree: regulation must be careful, measured, and informed by the science—not by political theater. While divisions remain, the shared concern over AI’s risks may offer a rare opportunity for consensus in Washington.\n",
    "\"\"\"],\n",
    "[\"\"\"In yet another blow to working-class Americans, Senate Republicans have blocked legislation that would raise the federal minimum wage to $17 per hour by 2027. With wages stagnant and inflation hitting food, rent, and transportation costs, the move is being widely condemned by labor leaders and economists.\n",
    "The current $7.25 minimum wage has not been raised since 2009, despite historic gains in productivity and corporate profits. Over 60% of Americans support a raise, but Republican lawmakers claim it would “hurt small businesses”—an argument that many economists say is overblown.\n",
    "In reality, the refusal to raise wages preserves exploitative systems where billion-dollar corporations rely on underpaid workers while CEO salaries skyrocket.\n",
    "This is not just about economics—it’s about dignity. Every American who works full-time should be able to afford basic necessities. Congress’s failure to act is a moral failure, and it’s up to voters to hold them accountable.\"\"\"],\n",
    " [\"\"\"The southern border has long been a flashpoint in American politics, but recent data shows that tougher enforcement and advanced surveillance technology are yielding results. Illegal crossings dropped 30% in the first quarter of 2025 compared to the previous year, according to Homeland Security reports.\n",
    "Under the new measures, authorities have deployed AI-powered drones, reinforced border fencing, and accelerated asylum screening procedures. Critics on the left say the policies are “inhumane,” but officials argue they are necessary to protect national sovereignty and public safety.\n",
    "Drug seizures have also increased, particularly fentanyl shipments originating from cartels that exploit weak border points. Law enforcement agencies say the new tools and funding are making a significant impact.\n",
    "The Biden administration was slow to act early in its term, but this policy shift marks a necessary correction. The right to immigrate must be balanced with the rule of law—and American citizens deserve to feel safe and secure in their own country.\n",
    "\"\"\"]\n",
    "]\n",
    " \n",
    "for text in samples:\n",
    "    output = test_pipeline(text, truncation=True, max_length=512)\n",
    "    sorted_output = sorted(output[0], key=lambda x: x[\"score\"], reverse=True)\n",
    "    top_label = sorted_output[0]\n",
    " \n",
    "    label_map = {\n",
    "    0: \"left\", 1: \"center\", 2: \"right\"}\n",
    "    # Extract the numeric part of the label like 'LABEL_1' -> 1\n",
    "    label_index = int(top_label['label'].split('_')[-1])\n",
    "    readable_label = label_map[label_index]\n",
    " \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Predicted Bias: {readable_label} \\n confidence score:({top_label['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555505f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aef6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
